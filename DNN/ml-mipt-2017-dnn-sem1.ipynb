{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h1> Deep Learning Seminar 1</h1>\n",
    "\n",
    "Credit cs231n.stanford.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">What was at Lecture?</h2>\n",
    "\n",
    "- Image Classification \n",
    "\n",
    "<img src=\"img/img-clf.png\" width=\"600\">\n",
    "\n",
    "- Linear Models (Что делает линейная модель простым языком)\n",
    "\n",
    "<img src=\"img/lm.png\" width=\"600\">\n",
    "<img src=\"img/lm-int.png\" width=\"600\">\n",
    "\n",
    "- Fully Connected Neural Nets\n",
    "\n",
    "<img src=\"img/fc-net.png\" width=\"600\">\n",
    "\n",
    "- Convolution Neural Nets (Какая мотивация при переходе к сверткам?)\n",
    "\n",
    "<img src=\"img/conv.png\" width=\"600\">\n",
    "\n",
    "- Зачем нужен backprop? \n",
    "\n",
    "<img src=\"img/bp.png\" width=\"600\">\n",
    "\n",
    "The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "\n",
    "- Что нужно накрутить на SGD чтобы получить хорошие методы стах оптимизации?\n",
    "\n",
    "<img src=\"img/adam.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">BackProp and Optimizers</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import check_grad\n",
    "from gradient_check import eval_numerical_gradient_array\n",
    "\n",
    "def rel_error(x, y):\n",
    "      return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Grad Check</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"img/gc.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Softmax Loss Layer</h3>\n",
    "<img src=\"img/loss.png\" width=\"300\">\n",
    "<img src=\"img/log.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " def softmax_loss(x, y):\n",
    "    \"\"\"\n",
    "    Computes the loss and gradient for softmax classification.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data, of shape (N, C) where x[i, j] is the score for the jth class\n",
    "    for the ith input.\n",
    "    - y: Vector of labels, of shape (N,) where y[i] is the label for x[i] and\n",
    "    0 <= y[i] < C\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - loss: Scalar giving the loss\n",
    "    - dx: Gradient of the loss with respect to x\n",
    "    \"\"\"\n",
    "    probabilities = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    probabilities /= np.sum(probabilities, axis=1, keepdims=True)\n",
    "    \n",
    "    loss = -np.sum(np.log(probabilities[np.arange(x.shape[0]), y]+1e-8)) / x.shape[0]\n",
    "\n",
    "    dx = probabilities.copy()\n",
    "    dx[np.arange(x.shape[0]), y] -= 1\n",
    "    dx /= x.shape[0]\n",
    "    \n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.random.randint(0, 3, 10)\n",
    "dx = lambda x: softmax_loss(x.reshape((10, 3)), y)[1].reshape(-1)\n",
    "loss = lambda x: softmax_loss(x.reshape((10, 3)), y)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss is a scalar\n",
      " 1.20140256775\n"
     ]
    }
   ],
   "source": [
    "print ('loss is a scalar\\n', loss(np.random.random((10, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient is a matrix with shape 10x3\n",
      " [ 0.03396904 -0.06836988  0.03440084  0.02437604 -0.0645053   0.04012926\n",
      "  0.03196209  0.03743329 -0.06939538  0.02297342 -0.05816115  0.03518774\n",
      " -0.05976153  0.0299827   0.02977883  0.04129687 -0.06410454  0.02280767\n",
      "  0.03095398  0.03322308 -0.06417706  0.0228719   0.04003603 -0.06290793\n",
      "  0.02839834  0.02889728 -0.05729563  0.04029172 -0.07095182  0.0306601 ]\n"
     ]
    }
   ],
   "source": [
    "print ('gradient is a matrix with shape 10x3\\n', dx(np.random.random((10, 3))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "difference should be ~10e-8 4.06917386559e-08\n"
     ]
    }
   ],
   "source": [
    "print ('difference should be ~10e-8', check_grad(loss, dx, np.random.random((10, 3)).reshape(-1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Dense Layer</h3>\n",
    "<img src=\"img/lin.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affine_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for an affine (fully-connected) layer.\n",
    "\n",
    "    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N\n",
    "    examples, where each example x[i] has shape (d_1, ..., d_k). We will\n",
    "    reshape each input into a vector of dimension D = d_1 * ... * d_k, and\n",
    "    then transform it to an output vector of dimension M.\n",
    "\n",
    "    Inputs:\n",
    "    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)\n",
    "    - w: A numpy array of weights, of shape (D, M)\n",
    "    - b: A numpy array of biases, of shape (M,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: output, of shape (N, M)\n",
    "    - cache: (x, w, b)\n",
    "    \"\"\"\n",
    "    out = None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine forward pass. Store the result in out. You     #\n",
    "    # will need to reshape the input into rows.                                 #\n",
    "    #############################################################################\n",
    "    newx = x.reshape((x.shape[0],-1))\n",
    "    out = newx.dot(w) + b\n",
    "    cache = (x, w, b)\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.76985004799e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print ('Testing affine_forward function:')\n",
    "print ('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def affine_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for an affine layer.\n",
    "\n",
    "    Inputs:\n",
    "    - dout: Upstream derivative, of shape (N, M)\n",
    "    - cache: Tuple of:\n",
    "    - x: Input data, of shape (N, d_1, ... d_k)\n",
    "    - w: Weights, of shape (D, M)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)\n",
    "    - dw: Gradient with respect to w, of shape (D, M)\n",
    "    - db: Gradient with respect to b, of shape (M,)\n",
    "    \"\"\"\n",
    "    x, w, b = cache\n",
    "    dx, dw, db = None, None, None\n",
    "    #############################################################################\n",
    "    # TODO: Implement the affine backward pass.                                 #\n",
    "    #############################################################################\n",
    "    \n",
    "    D = np.prod(x.shape[1:])\n",
    "    x_1 = np.reshape(x, (x.shape[0], D))\n",
    "\n",
    "    dx_1 = np.dot(dout, w.T) \n",
    "    dw = np.dot(x_1.T, dout) \n",
    "    db = np.dot(dout.T, np.ones(x.shape[0]))\n",
    "\n",
    "    dx = np.reshape(dx_1, x.shape)\n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_backward function:\n",
      "dx error:  1.4557347183e-10\n",
      "dw error:  3.18876111191e-10\n",
      "db error:  2.84536185975e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print ('Testing affine_backward function:')\n",
    "print ('dx error: ', rel_error(dx_num, dx))\n",
    "print ('dw error: ', rel_error(dw_num, dw))\n",
    "print ('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>ReLu Layer</h3>\n",
    "\n",
    "$$ReLu(x) = max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Computes the forward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - x: Inputs, of any shape\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - out: Output, of the same shape as x\n",
    "    - cache: x\n",
    "    \"\"\"\n",
    "    out = (x>0) * x\n",
    "    cache = x\n",
    "    return out, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.99999979802e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-8\n",
    "print ('Testing relu_forward function:')\n",
    "print ('difference: ', rel_error(out, correct_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Computes the backward pass for a layer of rectified linear units (ReLUs).\n",
    "\n",
    "    Input:\n",
    "    - dout: Upstream derivatives, of any shape\n",
    "    - cache: Input x, of same shape as dout\n",
    "\n",
    "    Returns:\n",
    "    - dx: Gradient with respect to x\n",
    "    \"\"\"\n",
    "    x = cache\n",
    "    dx = dout * (x >= 0) \n",
    "    dx = dx.reshape(*x.shape)\n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.27562668013e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print ('Testing relu_backward function:')\n",
    "print ('dx error: ', rel_error(dx_num, dx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Two Layer Fully Connected Neural Net with SGD</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%pylab inline\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f4e50105240>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPcAAAD7CAYAAAC2TgIoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4e5675ff28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pylab.imshow(X[5].reshape((8, 8)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0:\n",
      "\t tr_loss 16.76\n",
      "\t te_loss 15.95\n",
      "\t te_acc 0.101851851852\n",
      "epoch 1000:\n",
      "\t tr_loss 3.81\n",
      "\t te_loss 3.75\n",
      "\t te_acc 0.698148148148\n",
      "epoch 2000:\n",
      "\t tr_loss 2.57\n",
      "\t te_loss 2.33\n",
      "\t te_acc 0.805555555556\n",
      "epoch 3000:\n",
      "\t tr_loss 1.48\n",
      "\t te_loss 1.56\n",
      "\t te_acc 0.857407407407\n",
      "epoch 4000:\n",
      "\t tr_loss 1.16\n",
      "\t te_loss 1.39\n",
      "\t te_acc 0.875925925926\n",
      "epoch 5000:\n",
      "\t tr_loss 1.08\n",
      "\t te_loss 1.20\n",
      "\t te_acc 0.892592592593\n",
      "epoch 6000:\n",
      "\t tr_loss 0.49\n",
      "\t te_loss 1.04\n",
      "\t te_acc 0.901851851852\n",
      "epoch 7000:\n",
      "\t tr_loss 0.93\n",
      "\t te_loss 1.19\n",
      "\t te_acc 0.890740740741\n",
      "epoch 8000:\n",
      "\t tr_loss 0.25\n",
      "\t te_loss 1.08\n",
      "\t te_acc 0.909259259259\n",
      "epoch 9000:\n",
      "\t tr_loss 0.37\n",
      "\t te_loss 0.91\n",
      "\t te_acc 0.905555555556\n",
      "epoch 10000:\n",
      "\t tr_loss 0.21\n",
      "\t te_loss 0.98\n",
      "\t te_acc 0.9\n",
      "epoch 11000:\n",
      "\t tr_loss 0.19\n",
      "\t te_loss 0.92\n",
      "\t te_acc 0.911111111111\n",
      "epoch 12000:\n",
      "\t tr_loss 0.09\n",
      "\t te_loss 0.95\n",
      "\t te_acc 0.907407407407\n",
      "epoch 13000:\n",
      "\t tr_loss 0.53\n",
      "\t te_loss 1.13\n",
      "\t te_acc 0.901851851852\n",
      "epoch 14000:\n",
      "\t tr_loss 0.33\n",
      "\t te_loss 0.82\n",
      "\t te_acc 0.918518518519\n",
      "epoch 15000:\n",
      "\t tr_loss 0.30\n",
      "\t te_loss 0.84\n",
      "\t te_acc 0.911111111111\n",
      "epoch 16000:\n",
      "\t tr_loss 0.34\n",
      "\t te_loss 0.91\n",
      "\t te_acc 0.914814814815\n",
      "epoch 17000:\n",
      "\t tr_loss 0.24\n",
      "\t te_loss 0.90\n",
      "\t te_acc 0.912962962963\n",
      "epoch 18000:\n",
      "\t tr_loss 0.01\n",
      "\t te_loss 0.84\n",
      "\t te_acc 0.911111111111\n",
      "epoch 19000:\n",
      "\t tr_loss 0.09\n",
      "\t te_loss 0.85\n",
      "\t te_acc 0.901851851852\n",
      "epoch 20000:\n",
      "\t tr_loss 0.18\n",
      "\t te_loss 0.84\n",
      "\t te_acc 0.914814814815\n",
      "epoch 21000:\n",
      "\t tr_loss 0.18\n",
      "\t te_loss 0.93\n",
      "\t te_acc 0.905555555556\n",
      "epoch 22000:\n",
      "\t tr_loss 0.02\n",
      "\t te_loss 0.85\n",
      "\t te_acc 0.909259259259\n",
      "epoch 23000:\n",
      "\t tr_loss 0.10\n",
      "\t te_loss 0.84\n",
      "\t te_acc 0.914814814815\n",
      "epoch 24000:\n",
      "\t tr_loss 0.20\n",
      "\t te_loss 0.80\n",
      "\t te_acc 0.916666666667\n",
      "epoch 25000:\n",
      "\t tr_loss 0.13\n",
      "\t te_loss 0.85\n",
      "\t te_acc 0.909259259259\n",
      "epoch 26000:\n",
      "\t tr_loss 0.01\n",
      "\t te_loss 0.82\n",
      "\t te_acc 0.912962962963\n",
      "epoch 27000:\n",
      "\t tr_loss 0.13\n",
      "\t te_loss 0.81\n",
      "\t te_acc 0.922222222222\n",
      "epoch 28000:\n",
      "\t tr_loss 0.09\n",
      "\t te_loss 0.83\n",
      "\t te_acc 0.916666666667\n",
      "epoch 29000:\n",
      "\t tr_loss 0.01\n",
      "\t te_loss 0.79\n",
      "\t te_acc 0.922222222222\n",
      "epoch 30000:\n",
      "\t tr_loss 0.02\n",
      "\t te_loss 0.80\n",
      "\t te_acc 0.918518518519\n",
      "epoch 31000:\n",
      "\t tr_loss 0.01\n",
      "\t te_loss 0.78\n",
      "\t te_acc 0.922222222222\n",
      "epoch 32000:\n",
      "\t tr_loss 0.05\n",
      "\t te_loss 0.80\n",
      "\t te_acc 0.918518518519\n",
      "epoch 33000:\n",
      "\t tr_loss 0.10\n",
      "\t te_loss 0.85\n",
      "\t te_acc 0.918518518519\n",
      "epoch 34000:\n",
      "\t tr_loss 0.01\n",
      "\t te_loss 0.79\n",
      "\t te_acc 0.918518518519\n",
      "epoch 35000:\n",
      "\t tr_loss 0.09\n",
      "\t te_loss 0.82\n",
      "\t te_acc 0.905555555556\n",
      "epoch 36000:\n",
      "\t tr_loss 0.04\n",
      "\t te_loss 0.79\n",
      "\t te_acc 0.92037037037\n",
      "epoch 37000:\n",
      "\t tr_loss 0.01\n",
      "\t te_loss 0.78\n",
      "\t te_acc 0.924074074074\n",
      "epoch 38000:\n",
      "\t tr_loss 0.00\n",
      "\t te_loss 0.78\n",
      "\t te_acc 0.92037037037\n",
      "epoch 39000:\n",
      "\t tr_loss 0.00\n",
      "\t te_loss 0.77\n",
      "\t te_acc 0.925925925926\n",
      "epoch 40000:\n",
      "\t tr_loss 0.00\n",
      "\t te_loss 0.77\n",
      "\t te_acc 0.925925925926\n",
      "epoch 41000:\n",
      "\t tr_loss 0.03\n",
      "\t te_loss 0.76\n",
      "\t te_acc 0.924074074074\n",
      "epoch 42000:\n",
      "\t tr_loss 0.01\n",
      "\t te_loss 0.77\n",
      "\t te_acc 0.922222222222\n",
      "epoch 43000:\n",
      "\t tr_loss 0.05\n",
      "\t te_loss 0.76\n",
      "\t te_acc 0.924074074074\n",
      "epoch 44000:\n",
      "\t tr_loss 0.00\n",
      "\t te_loss 0.77\n",
      "\t te_acc 0.92037037037\n",
      "epoch 45000:\n",
      "\t tr_loss 0.02\n",
      "\t te_loss 0.77\n",
      "\t te_acc 0.925925925926\n",
      "epoch 46000:\n",
      "\t tr_loss 0.00\n",
      "\t te_loss 0.76\n",
      "\t te_acc 0.924074074074\n",
      "epoch 47000:\n",
      "\t tr_loss 0.03\n",
      "\t te_loss 0.76\n",
      "\t te_acc 0.924074074074\n",
      "epoch 48000:\n",
      "\t tr_loss 0.03\n",
      "\t te_loss 0.77\n",
      "\t te_acc 0.925925925926\n",
      "epoch 49000:\n",
      "\t tr_loss 0.03\n",
      "\t te_loss 0.75\n",
      "\t te_acc 0.922222222222\n"
     ]
    }
   ],
   "source": [
    "W1, b1 = np.random.random((64, 100)), np.random.random(100)\n",
    "W2, b2 = np.random.random((100, 10)), np.random.random(10)\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "for i in range(50000):\n",
    "    batch_index = np.random.randint(0, X_train.shape[0], 100)\n",
    "    batch_X, batch_y = X_train[batch_index], y_train[batch_index]\n",
    "    \n",
    "    # ------------ Train ----------------- \n",
    "    # Forward Pass\n",
    "    out1, cache1 = affine_forward(batch_X, W1, b1) # Dense Layer\n",
    "    out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "    out3, cache3 = affine_forward(out2, W2, b2) # Dense Layer \n",
    "    tr_loss, dx = softmax_loss(out3, batch_y)      # Loss Layer \n",
    "    \n",
    "    # Backward Pass\n",
    "    dx, dW2, db2 = affine_backward(dx, cache3)\n",
    "    dx = relu_backward(dx, cache2)\n",
    "    dx, dW1, db1 = affine_backward(dx, cache1)\n",
    "    \n",
    "    # Updates\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    \n",
    "    # ------------ Test ----------------- \n",
    "    # Forward Pass\n",
    "    out1, cache1 = affine_forward(X_test, W1, b1) # Dense Layer\n",
    "    out2, cache2 = relu_forward(out1)              # ReLu Layer\n",
    "    out3, cache3 = affine_forward(out2, W2, b2) # Dense Layer \n",
    "    te_loss, dx = softmax_loss(out3, y_test)         # Loss Layer\n",
    "    \n",
    "    # Predict\n",
    "    probs = np.exp(out3 - np.max(out3, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    y_pred = np.argmax(probs, axis=1)\n",
    "    if i % 1000 == 0:\n",
    "        print ('epoch %s:' % i,) \n",
    "        print ('\\t tr_loss %.2f' % tr_loss,)\n",
    "        print ('\\t te_loss %.2f' % te_loss,)\n",
    "        print ('\\t te_acc %s' % accuracy_score(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 align=\"center\">What is the challenge? </h2>\n",
    "\n",
    "You will see in Assignment 1:\n",
    "- more layers and architectures (Dropout, Convolution, Pooling)\n",
    "- optimization (Momentum, Adam)\n",
    "- weight initialization \n",
    "- data augmentation \n",
    "- ...\n",
    "\n",
    "<img src=\"img/rw.png\" width=\"900\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
